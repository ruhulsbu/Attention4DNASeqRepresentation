{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "#import pandas as pd\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "# Read the Input File\n",
    "max_data_size = 1527294\n",
    "\n",
    "def read_input_file(file_path, label=-1):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    file_read = open(file_path, \"r\")\n",
    "    for line in file_read:\n",
    "        data = [int(i) for i in line.strip()]\n",
    "        x_data.append(data)\n",
    "        y_data.append(label)\n",
    "        #print(x_data[-1], y_data[-1])\n",
    "        if len(x_data) == max_data_size:\n",
    "            break\n",
    "    file_read.close()\n",
    "    print(\"Sequences Read: \", len(x_data))\n",
    "    return np.array(x_data), np.array(y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_pos, y_data_pos = read_input_file(os.path.join(root_dir, \"Attention4DNASeqRepresentation/dataset/gene_range_start_codon.txt\"), 1)\n",
    "\n",
    "original_neg_intergenic_data, original_neg_intergenic_label = read_input_file(os.path.join(root_dir, \"Attention4DNASeqRepresentation/dataset/intragenic_start_codon.txt\"), 0)\n",
    "original_neg_coding_data, original_neg_coding_label = read_input_file(os.path.join(root_dir, \"Attention4DNASeqRepresentation/dataset/coding_start_codon.txt\"), 0)\n",
    "\n",
    "x_data_neg = np.concatenate((original_neg_coding_data, original_neg_intergenic_data))\n",
    "y_data_neg = np.concatenate((original_neg_coding_label, original_neg_intergenic_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_size=1000, batch_size = 100):\n",
    "    root_dir = \"/mnt/scratch7/hirak/\"\n",
    "\n",
    "    x_data_pos = x_data_pos[:data_size]\n",
    "    y_data_pos = y_data_pos[:data_size]\n",
    "\n",
    "    x_data_neg = x_data_neg[:data_size]\n",
    "    y_data_neg = y_data_neg[:data_size]\n",
    "\n",
    "    np.random.shuffle(x_data_neg)\n",
    "    np.random.shuffle(x_data_pos)\n",
    "\n",
    "    train_index = int((len(x_data_pos) / batch_size) * 0.60 * batch_size)\n",
    "    eval_index = train_index + int((len(x_data_pos) / batch_size) * 0.20 * batch_size)\n",
    "    test_index = eval_index + int((len(x_data_pos) / batch_size) * 0.20 * batch_size)\n",
    "\n",
    "    print(\"train, eval, test = \", (train_index, eval_index, test_index))\n",
    "\n",
    "    #Process Negative Data\n",
    "\n",
    "    x_train = x_data_neg[0:train_index]\n",
    "    y_train = y_data_neg[0:train_index]\n",
    "\n",
    "    x_eval = x_data_neg[train_index:eval_index]\n",
    "    y_eval = y_data_neg[train_index:eval_index]\n",
    "\n",
    "    x_test = x_data_neg[eval_index:test_index]\n",
    "    y_test = y_data_neg[eval_index:test_index]\n",
    "\n",
    "    #Process Positive Data\n",
    "\n",
    "    x_train = np.append(x_train, x_data_pos[0:train_index], axis=0)\n",
    "    y_train = np.append(y_train, y_data_pos[0:train_index], axis=0)\n",
    "\n",
    "    x_eval = np.append(x_eval, x_data_pos[train_index:eval_index], axis=0)\n",
    "    y_eval = np.append(y_eval, y_data_pos[train_index:eval_index], axis=0)\n",
    "\n",
    "    x_test = np.append(x_test, x_data_pos[eval_index:test_index], axis=0)\n",
    "    y_test = np.append(y_test, y_data_pos[eval_index:test_index], axis=0)\n",
    "\n",
    "    print(\"Sanity Check: \", np.sum(y_train), np.sum(y_eval), np.sum(y_test))\n",
    "\n",
    "    return (x_train, y_train, x_eval, y_eval, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):#corrected batch faster\n",
    "    #(self, time_steps, embedding_dim, hidden_dim, vocab_size, tagset_size, mini_batch)\n",
    "    def __init__(self, vocab_size, embedding_dim, \\\n",
    "                 hidden_dim, device, batch_size=100, debug=1, \\\n",
    "                 tagset_size=1, time_steps=101):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.time_steps = time_steps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.minibatch_size = batch_size\n",
    "        self.dropout_p = 0.25\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.debug = debug\n",
    "        self.device = device \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_one = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout_one = nn.Dropout(0.25)\n",
    "        self.lstm_two = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout_two = nn.Dropout(0.25)\n",
    "\n",
    "        self.attn_array = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(time_steps)])\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        self.attn_combine = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        #embedding_dim*time_steps\n",
    "        \"\"\"\n",
    "\n",
    "        self.hidden2tag_one = nn.Linear(hidden_dim*time_steps, 512)\n",
    "        self.dropout_three = nn.Dropout(0.25)\n",
    "        self.hidden2tag_two = nn.Linear(512, 128)\n",
    "        self.dropout_four = nn.Dropout(0.25)\n",
    "        self.hidden2tag_three = nn.Linear(128, 64)\n",
    "        self.dropout_five = nn.Dropout(0.25)\n",
    "\n",
    "        self.output = nn.Linear(64, tagset_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        init_embed = self.embedding(input)\n",
    "        #embedded = init_embed.permute(1, 0, 2)\n",
    "        if self.debug == 1:\n",
    "            print(\"Embedding Shape: \", init_embed.shape)\n",
    "\n",
    "        lstm_out, self.hidden_one = self.lstm_one(init_embed, self.hidden)\n",
    "        lstm_out = self.dropout_one(lstm_out)\n",
    "        lstm_out, self.hidden_two = self.lstm_two(lstm_out, self.hidden)\n",
    "        lstm_out = self.dropout_two(lstm_out)\n",
    "        #\"\"\"\n",
    "        lstm_permute = lstm_out.permute(1, 0, 2)\n",
    "        if self.debug == 1:\n",
    "            print(\"LSTM Out Shape: \", lstm_permute.shape)\n",
    "\n",
    "        attention = [self.attn_array[i](lstm_permute[i][:]) for i in range(self.time_steps)]\n",
    "        attention = torch.stack(attention)\n",
    "        attention.to(device)\n",
    "        \n",
    "        attention = attention.permute(1, 0, 2)\n",
    "        if self.debug == 1:\n",
    "            print(\"Attention Shape: \", attention.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        attn_weights = F.softmax(attention, dim=2)\n",
    "        #attn_weights = attn_weights.view(self.minibatch_size, self.time_steps, 1)\n",
    "        if self.debug == 1:\n",
    "            print(\"Softmax Shape: \", attn_weights.shape)\n",
    "        \"\"\"\n",
    "        attn_weights = torch.stack(\n",
    "            [attn_weights]*self.embedding_dim, 2).view(\n",
    "            self.minibatch_size, self.time_steps, -1)\n",
    "        if self.debug == 1:\n",
    "            print(\"Softmax ReShape: \", attn_weights.shape)\n",
    "        \"\"\"\n",
    "        #attn_applied = init_embed\n",
    "        attn_applied = attn_weights * init_embed\n",
    "        #attn_applied = attn_applied.view(self.minibatch_size, self.time_steps, -1)\n",
    "        #attn_applied = torch.sum(attn_applied, dim=1)\n",
    "        if self.debug == 1:\n",
    "            print(\"Embedding*Attention Shape: \", attn_applied.shape)\n",
    "\n",
    "        #output = F.relu(attn_applied)\n",
    "        #\"\"\"\n",
    "\n",
    "        lstm_out = attn_applied.contiguous().view(self.minibatch_size, -1)\n",
    "        #lstm_output = lstm_out.contiguous().view(self.minibatch_size, -1)\n",
    "        if self.debug == 1:\n",
    "            print(\"LSTM Output Shape: \", lstm_out.shape)\n",
    "\n",
    "\n",
    "        dense_out = self.hidden2tag_one(lstm_out[:])\n",
    "        dense_out = F.relu(dense_out[:])\n",
    "        dense_out = self.dropout_three(dense_out[:])\n",
    "\n",
    "        dense_out = self.hidden2tag_two(dense_out[:])\n",
    "        dense_out = F.relu(dense_out[:])\n",
    "        dense_out = self.dropout_four(dense_out[:])\n",
    "\n",
    "        dense_out = self.hidden2tag_three(dense_out[:])\n",
    "        dense_out = F.relu(dense_out[:])\n",
    "        dense_out = self.dropout_five(dense_out[:])\n",
    "\n",
    "        tag_space = self.output(dense_out[:])\n",
    "        #print(tag_space.shape)\n",
    "        #tag_scores = F.sigmoid(tag_space)\n",
    "        #tag_scores = F.softmax(tag_space, dim=1)\n",
    "        #print(tag_scores.shape)\n",
    "        return tag_space, attn_applied\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, self.minibatch_size, self.hidden_dim, device = device),\n",
    "                torch.zeros(1, self.minibatch_size, self.hidden_dim, device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences Read:  1527294\n",
      "Sequences Read:  1527294\n"
     ]
    }
   ],
   "source": [
    "data_size = 1000\n",
    "batch_size = 100\n",
    "\n",
    "x_train, y_train, x_eval, y_eval, x_test, y_test = load_data(data_size, batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 101), (400, 101), (400, 101))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_eval.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hirak/miniconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:1380: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.554, Accuracy: 0.338, Test Accuracy 0.000\n",
      "Epoch 2/100, Loss: 8.142, Accuracy: 0.537, Test Accuracy 0.300\n",
      "Epoch 3/100, Loss: 8.259, Accuracy: 0.649, Test Accuracy 0.800\n",
      "Epoch 4/100, Loss: 8.274, Accuracy: 0.627, Test Accuracy 0.940\n",
      "Epoch 5/100, Loss: 8.248, Accuracy: 0.662, Test Accuracy 0.790\n",
      "Epoch 6/100, Loss: 8.194, Accuracy: 0.705, Test Accuracy 0.810\n",
      "Epoch 7/100, Loss: 8.114, Accuracy: 0.726, Test Accuracy 0.820\n",
      "Epoch 8/100, Loss: 7.949, Accuracy: 0.738, Test Accuracy 0.840\n",
      "Epoch 9/100, Loss: 7.648, Accuracy: 0.758, Test Accuracy 0.830\n",
      "Epoch 10/100, Loss: 6.901, Accuracy: 0.808, Test Accuracy 0.870\n",
      "Epoch 11/100, Loss: 5.817, Accuracy: 0.849, Test Accuracy 0.890\n",
      "Epoch 12/100, Loss: 4.529, Accuracy: 0.894, Test Accuracy 0.890\n",
      "Epoch 13/100, Loss: 3.475, Accuracy: 0.911, Test Accuracy 0.900\n",
      "Epoch 14/100, Loss: 2.822, Accuracy: 0.923, Test Accuracy 0.930\n",
      "Epoch 15/100, Loss: 2.213, Accuracy: 0.948, Test Accuracy 0.920\n",
      "Epoch 16/100, Loss: 1.587, Accuracy: 0.960, Test Accuracy 0.920\n",
      "Epoch 17/100, Loss: 1.201, Accuracy: 0.969, Test Accuracy 0.950\n",
      "Epoch 18/100, Loss: 1.387, Accuracy: 0.968, Test Accuracy 0.920\n",
      "Epoch 19/100, Loss: 0.657, Accuracy: 0.989, Test Accuracy 0.940\n",
      "Epoch 20/100, Loss: 0.451, Accuracy: 0.993, Test Accuracy 0.940\n",
      "Epoch 21/100, Loss: 0.478, Accuracy: 0.992, Test Accuracy 0.900\n",
      "Epoch 22/100, Loss: 0.276, Accuracy: 0.994, Test Accuracy 0.920\n",
      "Epoch 23/100, Loss: 0.191, Accuracy: 0.997, Test Accuracy 0.940\n",
      "Epoch 24/100, Loss: 0.199, Accuracy: 0.995, Test Accuracy 0.940\n",
      "Epoch 25/100, Loss: 0.131, Accuracy: 0.998, Test Accuracy 0.930\n",
      "Epoch 26/100, Loss: 0.132, Accuracy: 0.997, Test Accuracy 0.940\n",
      "Epoch 27/100, Loss: 0.205, Accuracy: 0.995, Test Accuracy 0.850\n",
      "Epoch 28/100, Loss: 0.062, Accuracy: 0.999, Test Accuracy 0.940\n",
      "Epoch 29/100, Loss: 0.211, Accuracy: 0.993, Test Accuracy 0.920\n",
      "Epoch 30/100, Loss: 0.079, Accuracy: 0.998, Test Accuracy 0.940\n",
      "Epoch 31/100, Loss: 0.103, Accuracy: 0.997, Test Accuracy 0.900\n",
      "Epoch 32/100, Loss: 0.047, Accuracy: 0.998, Test Accuracy 0.930\n",
      "Epoch 33/100, Loss: 0.135, Accuracy: 0.996, Test Accuracy 0.910\n",
      "Epoch 34/100, Loss: 0.068, Accuracy: 0.998, Test Accuracy 0.930\n",
      "Epoch 35/100, Loss: 0.070, Accuracy: 0.999, Test Accuracy 0.920\n",
      "Epoch 36/100, Loss: 0.014, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 37/100, Loss: 0.033, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 38/100, Loss: 0.032, Accuracy: 0.999, Test Accuracy 0.910\n",
      "Epoch 39/100, Loss: 0.020, Accuracy: 0.999, Test Accuracy 0.910\n",
      "Epoch 40/100, Loss: 0.012, Accuracy: 1.000, Test Accuracy 0.890\n",
      "Epoch 41/100, Loss: 0.036, Accuracy: 0.998, Test Accuracy 0.910\n",
      "Epoch 42/100, Loss: 0.025, Accuracy: 0.999, Test Accuracy 0.890\n",
      "Epoch 43/100, Loss: 0.036, Accuracy: 0.999, Test Accuracy 0.890\n",
      "Epoch 44/100, Loss: 0.008, Accuracy: 1.000, Test Accuracy 0.900\n",
      "Epoch 45/100, Loss: 0.009, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 46/100, Loss: 0.026, Accuracy: 0.999, Test Accuracy 0.920\n",
      "Epoch 47/100, Loss: 0.023, Accuracy: 0.999, Test Accuracy 0.910\n",
      "Epoch 48/100, Loss: 0.070, Accuracy: 0.998, Test Accuracy 0.910\n",
      "Epoch 49/100, Loss: 0.006, Accuracy: 1.000, Test Accuracy 0.950\n",
      "Epoch 50/100, Loss: 0.007, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 51/100, Loss: 0.005, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 52/100, Loss: 0.010, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 53/100, Loss: 0.060, Accuracy: 0.998, Test Accuracy 0.880\n",
      "Epoch 54/100, Loss: 0.012, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 55/100, Loss: 0.037, Accuracy: 0.999, Test Accuracy 0.890\n",
      "Epoch 56/100, Loss: 0.034, Accuracy: 0.998, Test Accuracy 0.930\n",
      "Epoch 57/100, Loss: 0.020, Accuracy: 1.000, Test Accuracy 0.900\n",
      "Epoch 58/100, Loss: 0.018, Accuracy: 0.999, Test Accuracy 0.930\n",
      "Epoch 59/100, Loss: 0.015, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 60/100, Loss: 0.111, Accuracy: 0.997, Test Accuracy 0.910\n",
      "Epoch 61/100, Loss: 0.043, Accuracy: 0.998, Test Accuracy 0.950\n",
      "Epoch 62/100, Loss: 0.038, Accuracy: 0.998, Test Accuracy 0.930\n",
      "Epoch 63/100, Loss: 0.016, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 64/100, Loss: 0.029, Accuracy: 0.999, Test Accuracy 0.950\n",
      "Epoch 65/100, Loss: 0.044, Accuracy: 0.999, Test Accuracy 0.930\n",
      "Epoch 66/100, Loss: 0.095, Accuracy: 0.998, Test Accuracy 0.930\n",
      "Epoch 67/100, Loss: 0.099, Accuracy: 0.998, Test Accuracy 0.950\n",
      "Epoch 68/100, Loss: 0.008, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 69/100, Loss: 0.057, Accuracy: 0.997, Test Accuracy 0.930\n",
      "Epoch 70/100, Loss: 0.005, Accuracy: 1.000, Test Accuracy 0.950\n",
      "Epoch 71/100, Loss: 0.046, Accuracy: 0.999, Test Accuracy 0.920\n",
      "Epoch 72/100, Loss: 0.011, Accuracy: 1.000, Test Accuracy 0.960\n",
      "Epoch 73/100, Loss: 0.022, Accuracy: 0.999, Test Accuracy 0.900\n",
      "Epoch 74/100, Loss: 0.017, Accuracy: 1.000, Test Accuracy 0.900\n",
      "Epoch 75/100, Loss: 0.003, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 76/100, Loss: 0.005, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 77/100, Loss: 0.002, Accuracy: 1.000, Test Accuracy 0.920\n",
      "Epoch 78/100, Loss: 0.002, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 79/100, Loss: 0.005, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 80/100, Loss: 0.002, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 81/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 82/100, Loss: 0.003, Accuracy: 1.000, Test Accuracy 0.900\n",
      "Epoch 83/100, Loss: 0.003, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 84/100, Loss: 0.006, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 85/100, Loss: 0.007, Accuracy: 1.000, Test Accuracy 0.920\n",
      "Epoch 86/100, Loss: 0.003, Accuracy: 1.000, Test Accuracy 0.920\n",
      "Epoch 87/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 88/100, Loss: 0.003, Accuracy: 1.000, Test Accuracy 0.920\n",
      "Epoch 89/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 90/100, Loss: 0.011, Accuracy: 0.999, Test Accuracy 0.930\n",
      "Epoch 91/100, Loss: 0.003, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 92/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 93/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 94/100, Loss: 0.002, Accuracy: 1.000, Test Accuracy 0.910\n",
      "Epoch 95/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 96/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.940\n",
      "Epoch 97/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.930\n",
      "Epoch 98/100, Loss: 0.001, Accuracy: 1.000, Test Accuracy 0.900\n",
      "Epoch 99/100, Loss: 0.018, Accuracy: 0.999, Test Accuracy 0.900\n",
      "Epoch 100/100, Loss: 0.015, Accuracy: 1.000, Test Accuracy 0.880\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = AttnDecoderRNN(5, 16, 16, device, batch_size=batch_size, debug=0)\n",
    "#model = model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "X = torch.from_numpy(np.array(x_train).astype(int))\n",
    "Y = torch.from_numpy(np.array(y_train).reshape(len(y_train),1).astype(np.int))\n",
    "\n",
    "X_test = torch.from_numpy(np.array(x_test).astype(int))\n",
    "Y_test = torch.from_numpy(np.array(y_test).reshape(len(y_test),1).astype(np.int))\n",
    "\n",
    "X, Y = X.to(device), Y.to(device)\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "accuracies = []\n",
    "for epoch in range(num_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for index in range(0, len(X), batch_size):\n",
    "        sentence = X[index : index+batch_size]#.reshape(len(X[0]))\n",
    "        tags = Y[index : index+batch_size]#.reshape(len(Y[0]))\n",
    "        sentence.to(device)\n",
    "        tags.to(device)\n",
    "        #print(sentence.shape, tags.shape)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        #targets = prepare_sequence(tags, tag_to_ix)\n",
    "        targets = tags.float().flatten()\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, attn_weight = model(sentence)\n",
    "        tag_scores = tag_scores.flatten()\n",
    "        #print(targets.shape, tag_scores.shape)\n",
    "\n",
    "        #neg_weight = batch_size / (batch_size-np.sum(data_label[index : index+batch_size]))\n",
    "        #pos_weight = batch_size / np.sum(data_label[index : index+batch_size])\n",
    "        #weights = torch.FloatTensor([neg_weight, pos_weight])\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(tag_scores, targets)\n",
    "        #loss = weighted_binary_cross_entropy(tag_scores, targets, weights=weights)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        acc = binary_accuracy(tag_scores, targets)\n",
    "        total_acc += acc\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1,num_epochs, loss.data[0], correct/x.shape[0]))\n",
    "\n",
    "\n",
    "    # run forward on this epoch\n",
    "    accuracy_test = []\n",
    "    for index in range(0, len(X_test), batch_size):\n",
    "        sentence = X_test[index : index+batch_size]#.reshape(len(X[0]))\n",
    "        tags = Y_test[index : index+batch_size]#.reshape(len(Y[0]))\n",
    "        sentence.to(device)\n",
    "        tags.to(device)\n",
    "        #model.hidden = model.init_hidden()\n",
    "        tag_scores_test, attn_weight_test = model(sentence)\n",
    "        accuracy_test.append(binary_accuracy(tag_scores_test.flatten(), tags.float().flatten()))\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(total_acc/(len(X)/batch_size))\n",
    "\n",
    "    #total_loss.backward()\n",
    "    #opt.step()\n",
    "\n",
    "    #print(epoch, total_loss)#, total_acc)\n",
    "    print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}, Test Accuracy {:.3f}\".format(epoch+1,num_epochs, losses[-1], accuracies[-1], accuracy_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

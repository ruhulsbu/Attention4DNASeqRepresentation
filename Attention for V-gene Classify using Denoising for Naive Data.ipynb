{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import random, h5py\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#Initialize the Program\n",
    "alphabet = \"NACGT.\"\n",
    "vocab_size = 6\n",
    "batch_size = 1000\n",
    "embedding_size = 4\n",
    "time_steps = 101\n",
    "category = 2\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_complement(sequence):\n",
    "    retseq = ''\n",
    "    for k in range(len(sequence)-1, -1, -1):\n",
    "        if sequence[k] == 'A':\n",
    "            retseq = retseq + 'T'\n",
    "        elif sequence[k] == 'T':\n",
    "            retseq = retseq + 'A'\n",
    "        elif sequence[k] == 'C':\n",
    "            retseq = retseq + 'G'\n",
    "        elif sequence[k] == 'G':\n",
    "            retseq = retseq + 'C'\n",
    "        else:\n",
    "            retseq = retseq + sequence[k]\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Revese:\")\n",
    "    print(sequence)\n",
    "    print(retseq)\n",
    "    print()\n",
    "    \"\"\"\n",
    "    return retseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Data:\n",
      "200000 507 200000\n",
      "200000 58\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing Data:\")\n",
    "file_read = open(\"../imgtvgene_sequence_classification/naive.txt\")\n",
    "\n",
    "source_sequence = []\n",
    "target_sequence = []\n",
    "\n",
    "max_count = 200000\n",
    "max_source_len = 0\n",
    "\n",
    "vgene_tag = []\n",
    "vgene_dic = {}\n",
    "vgenes = 0\n",
    "\n",
    "count = 0\n",
    "\n",
    "line = file_read.readline()\n",
    "for line in file_read:\n",
    "    split = line.strip().split(\" \")\n",
    "    #print(split)\n",
    "    \n",
    "    try:\n",
    "        source = [char_to_int[x] for x in split[2][1:-1]]\n",
    "        if max_source_len < len(source):\n",
    "            max_source_len = len(source)\n",
    "\n",
    "        seq = [char_to_int[x] for x in split[3][1:-1] if x in \"NACGT\"]\n",
    "        target = [x * int(i < len(seq)) for (i, x) in enumerate(source)]\n",
    "        \n",
    "        assert(len(target) == len(source))\n",
    "    except:\n",
    "        print(\"Exceptions\")\n",
    "        continue\n",
    "        \n",
    "    source_sequence.append(source)\n",
    "    target_sequence.append(target)\n",
    "    \n",
    "    vgene_name = split[-1][1:-1].split(\"*\")[0]\n",
    "    #* for Gene /- for Family /No Split for Allele\n",
    "    if not vgene_name in vgene_dic:\n",
    "        vgene_dic[vgene_name] = vgenes\n",
    "        vgenes += 1\n",
    "    vgene_tag.append(vgene_dic[vgene_name])\n",
    "    \n",
    "    count += 1\n",
    "    #print(split[1], vgene_name)\n",
    "    if count == max_count:\n",
    "        break\n",
    "    \n",
    "print(len(source_sequence), max_source_len, len(target_sequence))\n",
    "print(len(vgene_tag), vgenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86a1d88588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(vgene_tag)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 3, 4, 3, 2, 1, 3, 2, 4, 3, 3, 4, 3, 3, 1, 3, 4, 2, 4, 3,\n",
       "       3, 3, 3, 3, 1, 3, 3, 2, 3, 4, 3, 3, 4, 2, 2, 1, 3, 2, 2, 4, 3, 3,\n",
       "       3, 1, 3, 3, 4, 2, 2, 2, 4, 3, 1, 3, 1, 2, 4, 2, 4, 2, 2, 4, 3, 4,\n",
       "       3, 2, 1, 3, 2, 2, 4, 2, 4, 3, 3, 1, 4, 4, 2, 1, 2, 2, 4, 4, 2, 1,\n",
       "       3, 4, 1, 3, 2, 4, 1, 4, 3, 3, 2, 1, 4, 3, 2, 1, 2, 4, 3, 3, 3, 4,\n",
       "       2, 2, 3, 2, 2, 1, 3, 3, 2, 4, 2, 2, 1, 3, 3, 2, 1, 1, 3, 3, 3, 3,\n",
       "       2, 4, 3, 3, 1, 3, 4, 3, 3, 3, 4, 3, 3, 2, 1, 3, 4, 4, 1, 4, 1, 4,\n",
       "       2, 1, 4, 1, 4, 3, 1, 4, 3, 3, 1, 1, 3, 4, 1, 1, 4, 1, 1, 1, 4, 1,\n",
       "       2, 4, 1, 4, 3, 2, 1, 3, 1, 2, 4, 2, 2, 3, 4, 3, 1, 1, 3, 3, 3, 2,\n",
       "       2, 3, 1, 4, 4, 2, 1, 2, 2, 1, 4, 2, 4, 2, 2, 1, 3, 1, 3, 1, 2, 1,\n",
       "       1, 4, 4, 2, 2, 1, 1, 3, 1, 1, 2, 1, 2, 3, 2, 4, 3, 4, 1, 4, 2, 4,\n",
       "       3, 2, 1, 1, 1, 4, 3, 1, 1, 2, 1, 3, 2, 2, 4, 3, 1, 3, 1, 3, 2, 4,\n",
       "       3, 1, 3, 3, 1, 2, 1, 2, 3, 3, 2, 4, 3, 4, 3, 4, 1, 4, 4, 1, 2, 4,\n",
       "       3, 4, 3, 2, 3, 1, 1, 1, 3, 1, 4, 1, 1, 1, 2, 4, 1, 1, 4, 1, 3, 4,\n",
       "       3, 3, 3, 1, 3, 2, 4, 1, 2, 4, 3, 3, 1, 3, 2, 3, 4, 3, 3, 1, 1, 2,\n",
       "       4, 3, 3, 4, 4, 2, 3, 1, 2, 2, 2, 2, 4, 3, 3, 3, 3, 2, 2, 1, 3, 3,\n",
       "       3, 1, 1, 2, 2, 2, 4, 3, 3, 4, 2, 1, 2, 2, 3, 4, 2, 4, 2, 2, 4, 2,\n",
       "       1, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 3, 4, 3, 2, 1, 3, 2, 4, 3, 3, 4, 3, 3, 1, 3, 4, 2, 4, 3,\n",
       "       3, 3, 3, 3, 1, 3, 3, 2, 3, 4, 3, 3, 4, 2, 2, 1, 3, 2, 2, 4, 3, 3,\n",
       "       3, 1, 3, 3, 4, 2, 2, 2, 4, 3, 1, 3, 1, 2, 4, 2, 4, 2, 2, 4, 3, 4,\n",
       "       3, 2, 1, 3, 2, 2, 4, 2, 4, 3, 3, 1, 4, 4, 2, 1, 2, 2, 4, 4, 2, 1,\n",
       "       3, 4, 1, 3, 2, 4, 1, 4, 3, 3, 2, 1, 4, 3, 2, 1, 2, 4, 3, 3, 3, 4,\n",
       "       2, 2, 3, 2, 2, 1, 3, 3, 2, 4, 2, 2, 1, 3, 3, 2, 1, 1, 3, 3, 3, 3,\n",
       "       2, 4, 3, 3, 1, 3, 4, 3, 3, 3, 4, 3, 3, 2, 1, 3, 4, 4, 1, 4, 1, 4,\n",
       "       2, 1, 4, 1, 4, 3, 1, 4, 3, 3, 1, 1, 3, 4, 1, 1, 4, 1, 1, 1, 4, 1,\n",
       "       2, 4, 1, 4, 3, 2, 1, 3, 1, 2, 4, 2, 2, 3, 4, 3, 1, 1, 3, 3, 3, 2,\n",
       "       2, 3, 1, 4, 4, 2, 1, 2, 2, 1, 4, 2, 4, 2, 2, 1, 3, 1, 3, 1, 2, 1,\n",
       "       1, 4, 4, 2, 2, 1, 1, 3, 1, 1, 2, 1, 2, 3, 2, 4, 3, 4, 1, 4, 2, 4,\n",
       "       3, 2, 1, 1, 1, 4, 3, 1, 1, 2, 1, 3, 2, 2, 4, 3, 1, 3, 1, 3, 2, 4,\n",
       "       3, 1, 3, 3, 1, 2, 1, 2, 3, 3, 2, 4, 3, 4, 3, 4, 1, 4, 4, 1, 2, 4,\n",
       "       3, 4, 3, 2, 3, 1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 200000 200000\n",
      "(200000, 507) (200000, 58) (200000, 507)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "source_data = pad_sequences(source_sequence, maxlen=max_source_len, padding='post', value=0)\n",
    "classes = to_categorical(vgene_tag)\n",
    "target_data = pad_sequences(target_sequence, maxlen=max_source_len, padding='post', value=0)\n",
    "\n",
    "print(len(source_sequence), len(vgene_tag), len(target_sequence))\n",
    "print(source_data.shape, classes.shape, target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 4, 1, 2, 1, 3, 2, 4, 3, 2, 1, 3, 3, 1, 3, 4, 2, 3, 3,\n",
       "       3, 2, 2, 2, 1, 3, 3, 1, 2, 4, 3, 3, 4, 3, 1, 1, 3, 2, 2, 4, 4, 2,\n",
       "       3, 3, 1, 2, 1, 2, 2, 2, 4, 3, 4, 2, 2, 2, 4, 2, 1, 2, 2, 4, 3, 2,\n",
       "       3, 2, 4, 3, 4, 2, 4, 2, 4, 3, 3, 4, 4, 1, 2, 4, 2, 2, 1, 4, 2, 1,\n",
       "       3, 2, 1, 3, 4, 1, 3, 4, 1, 1, 2, 4, 3, 3, 4, 3, 3, 3, 3, 2, 4, 3,\n",
       "       3, 1, 4, 2, 2, 3, 3, 2, 1, 3, 2, 2, 2, 2, 2, 1, 3, 3, 3, 1, 1, 3,\n",
       "       3, 3, 1, 2, 4, 3, 3, 1, 3, 4, 3, 3, 1, 4, 4, 3, 3, 3, 4, 1, 2, 1,\n",
       "       4, 2, 4, 1, 4, 4, 1, 4, 1, 3, 4, 3, 3, 3, 1, 3, 2, 1, 2, 2, 4, 1,\n",
       "       2, 4, 1, 2, 1, 1, 2, 2, 2, 3, 4, 2, 2, 2, 4, 2, 1, 1, 3, 1, 3, 4,\n",
       "       2, 3, 1, 3, 4, 2, 1, 2, 2, 1, 4, 3, 4, 2, 1, 3, 4, 1, 3, 1, 2, 1,\n",
       "       2, 3, 4, 2, 2, 1, 1, 3, 1, 1, 2, 2, 1, 3, 4, 4, 2, 4, 2, 2, 2, 4,\n",
       "       3, 1, 1, 3, 2, 4, 3, 1, 3, 2, 4, 2, 4, 3, 4, 3, 1, 2, 2, 3, 2, 2,\n",
       "       3, 4, 3, 3, 1, 2, 1, 2, 3, 3, 2, 2, 3, 4, 3, 4, 1, 4, 4, 1, 2, 4,\n",
       "       3, 4, 3, 2, 3, 1, 3, 1, 1, 1, 1, 3, 3, 4, 1, 2, 4, 2, 3, 2, 3, 3,\n",
       "       3, 3, 3, 1, 4, 2, 2, 4, 4, 2, 3, 1, 4, 2, 4, 2, 4, 3, 3, 3, 3, 2,\n",
       "       2, 3, 4, 3, 3, 2, 1, 2, 2, 2, 4, 3, 3, 4, 2, 1, 2, 4, 3, 4, 2, 4,\n",
       "       2, 2, 4, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 4, 1, 2, 1, 3, 2, 4, 3, 2, 1, 3, 3, 1, 3, 4, 2, 3, 3,\n",
       "       3, 2, 2, 2, 1, 3, 3, 1, 2, 4, 3, 3, 4, 3, 1, 1, 3, 2, 2, 4, 4, 2,\n",
       "       3, 3, 1, 2, 1, 2, 2, 2, 4, 3, 4, 2, 2, 2, 4, 2, 1, 2, 2, 4, 3, 2,\n",
       "       3, 2, 4, 3, 4, 2, 4, 2, 4, 3, 3, 4, 4, 1, 2, 4, 2, 2, 1, 4, 2, 1,\n",
       "       3, 2, 1, 3, 4, 1, 3, 4, 1, 1, 2, 4, 3, 3, 4, 3, 3, 3, 3, 2, 4, 3,\n",
       "       3, 1, 4, 2, 2, 3, 3, 2, 1, 3, 2, 2, 2, 2, 2, 1, 3, 3, 3, 1, 1, 3,\n",
       "       3, 3, 1, 2, 4, 3, 3, 1, 3, 4, 3, 3, 1, 4, 4, 3, 3, 3, 4, 1, 2, 1,\n",
       "       4, 2, 4, 1, 4, 4, 1, 4, 1, 3, 4, 3, 3, 3, 1, 3, 2, 1, 2, 2, 4, 1,\n",
       "       2, 4, 1, 2, 1, 1, 2, 2, 2, 3, 4, 2, 2, 2, 4, 2, 1, 1, 3, 1, 3, 4,\n",
       "       2, 3, 1, 3, 4, 2, 1, 2, 2, 1, 4, 3, 4, 2, 1, 3, 4, 1, 3, 1, 2, 1,\n",
       "       2, 3, 4, 2, 2, 1, 1, 3, 1, 1, 2, 2, 1, 3, 4, 4, 2, 4, 2, 2, 2, 4,\n",
       "       3, 1, 1, 3, 2, 4, 3, 1, 3, 2, 4, 2, 4, 3, 4, 3, 1, 2, 2, 3, 2, 2,\n",
       "       3, 4, 3, 3, 1, 2, 1, 2, 3, 3, 2, 2, 3, 4, 3, 4, 1, 4, 4, 1, 2, 4,\n",
       "       3, 4, 3, 2, 3, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(sequence, n_unique):\n",
    "    encoding = np.zeros((len(sequence), n_unique))\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        encoding[i][sequence[i]] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(max(source_data[0]), max(target_data[0]))\n",
    "print(one_hot_encode(source_data[0], vocab_size))\n",
    "print(one_hot_encode(target_data[0], vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 507, 5)\n"
     ]
    }
   ],
   "source": [
    "source_encoded = np.array([one_hot_encode(seq, vocab_size-1) for seq in source_data[:]])\n",
    "print(source_encoded.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 507, 5)\n"
     ]
    }
   ],
   "source": [
    "target_encoded = np.array([one_hot_encode(seq, vocab_size-1) for seq in target_data[:]])\n",
    "print(target_encoded.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000.0 200000\n"
     ]
    }
   ],
   "source": [
    "print(sum(sum(classes[:])), len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the encoder-decoder model\n",
    "def baseline_model(n_timesteps_in, n_features, n_classes):\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(5, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "    #model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(Flatten(input_shape=(n_timesteps_in, n_features)))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2535)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2535)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2596864   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 58)                14906     \n",
      "=================================================================\n",
      "Total params: 3,267,898\n",
      "Trainable params: 3,267,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/3\n",
      " - 56s - loss: 0.1434 - acc: 0.9602 - val_loss: 0.1734 - val_acc: 0.9617\n",
      "Epoch 2/3\n",
      " - 55s - loss: 0.0315 - acc: 0.9915 - val_loss: 0.1557 - val_acc: 0.9673\n",
      "Epoch 3/3\n",
      " - 55s - loss: 0.0261 - acc: 0.9929 - val_loss: 0.1464 - val_acc: 0.9761\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86072d3710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from custom_recurrents import AttentionDecoder\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "train_data_size = 100000\n",
    "test_source_len = max_source_len\n",
    "model = baseline_model(test_source_len, vocab_size-1, len(classes[0]))\n",
    "\n",
    "model.fit(target_encoded[:train_data_size, :test_source_len, :], \\\n",
    "          classes[:train_data_size], callbacks=[TQDMNotebookCallback()], \\\n",
    "          validation_data=(source_encoded[train_data_size:, :test_source_len, :], \\\n",
    "          classes[train_data_size:]), batch_size=100, epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LEN = max_source_len # maximum sequence length\n",
    "DIM_ENC = 5 # dimension of a one-hot encoded vector (e.g., 4 (sequence) x 4 (structure) = 16)\n",
    "DIM_LSTM1 = 16\n",
    "DIM_LSTM2 = 16\n",
    "DIM_DENSE1 = 256\n",
    "DMI_DENSE2 = 128\n",
    "N_CLASSES = len(classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Reshape, Dense\n",
    "from keras.models import Model\n",
    "from deepMiRGene import SoftAttention\n",
    "\n",
    "# define the encoder-decoder with attention model\n",
    "def attention_model(MAX_LEN, N_CLASSES, DIM_ENC=5, \\\n",
    "                    DIM_LSTM1=16, DIM_LSTM2=16, \\\n",
    "                    DIM_DENSE1=256, DMI_DENSE2=128):\n",
    "    \n",
    "    inputs = Input(shape=(MAX_LEN,DIM_ENC), name='inputs')\n",
    "    msk = Masking(mask_value=0)(inputs)\n",
    "    lstm1 = LSTM(DIM_LSTM1, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(msk)\n",
    "    lstm2 = LSTM(DIM_LSTM2, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(lstm1)\n",
    "\n",
    "    att, pv = SoftAttention(lstm2)(lstm2)\n",
    "\n",
    "    do1 = Dropout(0.1)(att)\n",
    "    dense1 = Dense(DIM_DENSE1,activation='sigmoid')(do1)\n",
    "    do2 = Dropout(0.1)(dense1)\n",
    "    dense2 = Dense(DMI_DENSE2,activation='sigmoid')(do2)\n",
    "    outputs = Dense(N_CLASSES,activation='softmax')(dense2)\n",
    "\n",
    "    model=Model(outputs=outputs, inputs=inputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Mask:  Tensor(\"masking_1/Any_1:0\", shape=(?, 507), dtype=bool)\n",
      "Cast:  Tensor(\"soft_attention_1/Cast:0\", shape=(?, 507), dtype=float32)\n",
      "Return PT\n",
      "Vector:  Tensor(\"soft_attention_1/mul_1:0\", shape=(?, 507), dtype=float32)\n",
      "Vector:  Tensor(\"soft_attention_1/ExpandDims:0\", shape=(?, 507, 1), dtype=float32)\n",
      "Object:  Tensor(\"lstm_2/transpose_2:0\", shape=(?, ?, 16), dtype=float32) (None, 507, 16)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 507, 5)            0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 507, 5)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 507, 16)           1408      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 507, 16)           2112      \n",
      "_________________________________________________________________\n",
      "soft_attention_1 (SoftAttent [(None, 8112), (None, 507 17        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8112)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               2076928   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 58)                7482      \n",
      "=================================================================\n",
      "Total params: 2,120,843\n",
      "Trainable params: 2,120,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/10\n",
      " - 1433s - loss: 1.6397 - acc: 0.5694 - val_loss: 0.3578 - val_acc: 0.8954\n",
      "Epoch 2/10\n",
      " - 1386s - loss: 0.3868 - acc: 0.9049 - val_loss: 0.1686 - val_acc: 0.9538\n",
      "Epoch 3/10\n",
      " - 1386s - loss: 0.1719 - acc: 0.9587 - val_loss: 0.1822 - val_acc: 0.9484\n",
      "Epoch 4/10\n",
      " - 1379s - loss: 0.1097 - acc: 0.9737 - val_loss: 0.1306 - val_acc: 0.9629\n",
      "Epoch 5/10\n",
      " - 1384s - loss: 0.0790 - acc: 0.9815 - val_loss: 0.1308 - val_acc: 0.9624\n",
      "Epoch 6/10\n",
      " - 1386s - loss: 0.0620 - acc: 0.9854 - val_loss: 0.2249 - val_acc: 0.9402\n",
      "Epoch 7/10\n",
      " - 1477s - loss: 0.0507 - acc: 0.9883 - val_loss: 0.1376 - val_acc: 0.9660\n",
      "Epoch 8/10\n",
      " - 1418s - loss: 0.0439 - acc: 0.9900 - val_loss: 0.1555 - val_acc: 0.9543\n",
      "Epoch 9/10\n",
      " - 1438s - loss: 0.0384 - acc: 0.9910 - val_loss: 0.1007 - val_acc: 0.9724\n",
      "Epoch 10/10\n",
      " - 1459s - loss: 0.0336 - acc: 0.9921 - val_loss: 0.0698 - val_acc: 0.9746\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f839c671f28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from custom_recurrents import AttentionDecoder\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "train_data_size = 100000\n",
    "test_source_len = max_source_len\n",
    "model = attention_model(MAX_LEN, N_CLASSES)\n",
    "\n",
    "model.fit(target_encoded[:train_data_size, :test_source_len, :], \\\n",
    "          classes[:train_data_size], callbacks=[TQDMNotebookCallback()], \\\n",
    "          validation_data=(source_encoded[train_data_size:, :test_source_len, :], \\\n",
    "          classes[train_data_size:]), batch_size=100, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "1b0b4914d4db40399bbb1b449244605d": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "2b62b25b03624c18a5a348f16ad3866d": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "6dc078e6b8d24262bc918e5875539df4": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
